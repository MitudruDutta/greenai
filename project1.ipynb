{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# # Green AI Competition - Machine Learning Solution\n",
    "#\n",
    "# This notebook implements an advanced machine learning pipeline for the HACK4EARTH Green AI competition.\n",
    "#\n",
    "# ## Overview\n",
    "# - **Goal**: Build accurate predictions while maintaining green AI principles\n",
    "# - **Approach**: Feature engineering + ensemble learning\n",
    "# - **Models**: XGBoost, CatBoost, Random Forest, Gradient Boosting, Ridge, ElasticNet, and Stacking\n"
   ],
   "id": "a16f2c89ea14efb5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ## 1. Import Libraries\n",
    "#\n",
    "# Loading all necessary libraries for data processing, modeling, and evaluation.\n"
   ],
   "id": "e08ef0cdea181c21"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T16:36:26.637651Z",
     "start_time": "2025-10-16T16:36:25.595964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")\n"
   ],
   "id": "ac629f2742b5d9ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All libraries imported successfully\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ## 2. Load and Explore Data\n",
    "#\n",
    "# Loading the training, test, and metadata files to understand the dataset structure.\n"
   ],
   "id": "6819562a44d66c04"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T16:36:31.158051Z",
     "start_time": "2025-10-16T16:36:31.146662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"FINAL SOLUTION - FEATURE ENGINEERING + ADVANCED ML\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "metadata_df = pd.read_csv('metaData.csv')\n",
    "\n",
    "print(\"\\n[1] DATA ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTrain data shape: {train_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "print(f\"\\nTrain data:\\n{train_df}\")\n",
    "print(f\"\\nTest data:\\n{test_df}\")\n"
   ],
   "id": "fe20ce5a2ece4027",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FINAL SOLUTION - FEATURE ENGINEERING + ADVANCED ML\n",
      "================================================================================\n",
      "\n",
      "[1] DATA ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Train data shape: (5, 4)\n",
      "Test data shape: (3, 1)\n",
      "\n",
      "Train data:\n",
      "  example_id  feature_1  feature_2  target\n",
      "0      TR001       0.12         10     1.0\n",
      "1      TR002       0.34         12     0.0\n",
      "2      TR003       0.56          9     1.0\n",
      "3      TR004       0.78         13     0.0\n",
      "4      TR005       0.91         11     1.0\n",
      "\n",
      "Test data:\n",
      "  example_id\n",
      "0      TS001\n",
      "1      TS002\n",
      "2      TS003\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ## 3. Feature Inference (Scaffold Dataset Handling)\n",
    "#\n",
    "# Since this is a scaffold dataset, the test set may not have features. We need to infer them from patterns in the training data.\n",
    "#\n",
    "# **Strategy**: Analyze the relationship between IDs and features to predict test features.\n"
   ],
   "id": "d8c31549faa45fec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T16:36:38.070188Z",
     "start_time": "2025-10-16T16:36:38.057679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check if test has features\n",
    "has_test_features = len(test_df.columns) > 1\n",
    "\n",
    "if not has_test_features:\n",
    "    print(\"\\n⚠ WARNING: Test data has NO features!\")\n",
    "    print(\"This is a scaffold dataset. Need to infer test features from patterns.\")\n",
    "    print(\"\\nAnalyzing training data patterns to predict test features...\")\n",
    "\n",
    "    # Analyze pattern in training data\n",
    "    print(\"\\n[2] PATTERN ANALYSIS IN TRAINING DATA\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Extract ID numbers\n",
    "    train_df['id_num'] = train_df['example_id'].str.extract('(\\d+)').astype(int)\n",
    "    test_df['id_num'] = test_df['example_id'].str.extract('(\\d+)').astype(int)\n",
    "\n",
    "    print(f\"\\nTrain IDs: {train_df['id_num'].tolist()}\")\n",
    "    print(f\"Test IDs: {test_df['id_num'].tolist()}\")\n",
    "\n",
    "    # Analyze relationship between ID and features\n",
    "    print(f\"\\nID vs feature_1 correlation: {train_df['id_num'].corr(train_df['feature_1']):.4f}\")\n",
    "    print(f\"ID vs feature_2 correlation: {train_df['id_num'].corr(train_df['feature_2']):.4f}\")\n",
    "    print(f\"ID vs target correlation: {train_df['id_num'].corr(train_df['target']):.4f}\")\n",
    "\n",
    "    # Predict feature_1 for test based on ID pattern\n",
    "    lr_f1 = LinearRegression()\n",
    "    lr_f1.fit(train_df[['id_num']], train_df['feature_1'])\n",
    "    test_df['feature_1'] = lr_f1.predict(test_df[['id_num']])\n",
    "\n",
    "    # Predict feature_2 for test based on ID pattern\n",
    "    lr_f2 = LinearRegression()\n",
    "    lr_f2.fit(train_df[['id_num']], train_df['feature_2'])\n",
    "    test_df['feature_2'] = lr_f2.predict(test_df[['id_num']])\n",
    "\n",
    "    print(f\"\\n✓ Inferred test features from ID patterns:\")\n",
    "    print(test_df[['example_id', 'id_num', 'feature_1', 'feature_2']])\n",
    "else:\n",
    "    print(\"\\n✓ Test data has features - no inference needed\")\n"
   ],
   "id": "8b472810ad4a3a30",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚠ WARNING: Test data has NO features!\n",
      "This is a scaffold dataset. Need to infer test features from patterns.\n",
      "\n",
      "Analyzing training data patterns to predict test features...\n",
      "\n",
      "[2] PATTERN ANALYSIS IN TRAINING DATA\n",
      "================================================================================\n",
      "\n",
      "Train IDs: [1, 2, 3, 4, 5]\n",
      "Test IDs: [1, 2, 3]\n",
      "\n",
      "ID vs feature_1 correlation: 0.9961\n",
      "ID vs feature_2 correlation: 0.3000\n",
      "ID vs target correlation: 0.0000\n",
      "\n",
      "✓ Inferred test features from ID patterns:\n",
      "  example_id  id_num  feature_1  feature_2\n",
      "0      TS001       1      0.138       10.4\n",
      "1      TS002       2      0.340       10.7\n",
      "2      TS003       3      0.542       11.0\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ## 4. Feature Engineering Function\n",
    "#\n",
    "# Creating advanced features from the base features:\n",
    "# - **Polynomial features**: squared, cubed\n",
    "# - **Interaction features**: multiplication, ratios\n",
    "# - **Statistical features**: sum, difference, mean, std\n",
    "# - **Logarithmic features**: log transformations\n",
    "# - **Trigonometric features**: sine, cosine transformations\n",
    "# - **Exponential features**: exponential transformations\n"
   ],
   "id": "61c415b8359d5aa5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T16:36:41.070419Z",
     "start_time": "2025-10-16T16:36:41.065423Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\n[3] ADVANCED FEATURE ENGINEERING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"\n",
    "    Create advanced engineered features from base features.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with at least 'feature_1' and 'feature_2' columns\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with additional engineered features\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Basic features\n",
    "    f1 = df['feature_1']\n",
    "    f2 = df['feature_2']\n",
    "\n",
    "    # Polynomial features\n",
    "    df['f1_squared'] = f1 ** 2\n",
    "    df['f2_squared'] = f2 ** 2\n",
    "    df['f1_cubed'] = f1 ** 3\n",
    "    df['f2_cubed'] = f2 ** 3\n",
    "\n",
    "    # Interaction features\n",
    "    df['f1_f2_interaction'] = f1 * f2\n",
    "    df['f1_f2_ratio'] = f1 / (f2 + 1e-5)\n",
    "    df['f2_f1_ratio'] = f2 / (f1 + 1e-5)\n",
    "\n",
    "    # Statistical features\n",
    "    df['f1_f2_sum'] = f1 + f2\n",
    "    df['f1_f2_diff'] = f1 - f2\n",
    "    df['f1_f2_mean'] = (f1 + f2) / 2\n",
    "    df['f1_f2_std'] = ((f1 - df['f1_f2_mean'])**2 + (f2 - df['f1_f2_mean'])**2)**0.5\n",
    "\n",
    "    # Logarithmic features\n",
    "    df['f1_log'] = np.log1p(np.abs(f1))\n",
    "    df['f2_log'] = np.log1p(f2)\n",
    "\n",
    "    # Trigonometric features (treating as angles)\n",
    "    df['f1_sin'] = np.sin(f1 * np.pi)\n",
    "    df['f1_cos'] = np.cos(f1 * np.pi)\n",
    "    df['f2_sin'] = np.sin(f2 / 14 * np.pi)  # Normalize by max\n",
    "    df['f2_cos'] = np.cos(f2 / 14 * np.pi)\n",
    "\n",
    "    # Exponential features\n",
    "    df['f1_exp'] = np.exp(f1) / 100  # Scale down\n",
    "    df['f2_exp'] = np.exp(f2 / 10)\n",
    "\n",
    "    return df\n",
    "\n",
    "print(\"✓ Feature engineering function defined\")\n"
   ],
   "id": "10833e070a40c04a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3] ADVANCED FEATURE ENGINEERING\n",
      "================================================================================\n",
      "✓ Feature engineering function defined\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ## 5. Apply Feature Engineering\n",
    "#\n",
    "# Applying the feature engineering function to both training and test datasets.\n"
   ],
   "id": "523b08cc8c8cace1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T16:36:43.693124Z",
     "start_time": "2025-10-16T16:36:43.680705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_engineered = engineer_features(train_df)\n",
    "test_engineered = engineer_features(test_df)\n",
    "\n",
    "# Select engineered features\n",
    "engineered_features = [col for col in train_engineered.columns\n",
    "                       if col not in ['example_id', 'target', 'id_num']]\n",
    "\n",
    "print(f\"\\n✓ Created {len(engineered_features)} engineered features:\")\n",
    "for i, feat in enumerate(engineered_features, 1):\n",
    "    print(f\"  {i:2d}. {feat}\")\n",
    "\n",
    "# Prepare data\n",
    "X = train_engineered[engineered_features]\n",
    "y = train_df['target']\n",
    "X_test = test_engineered[engineered_features]\n",
    "\n",
    "print(f\"\\n📊 Data shapes:\")\n",
    "print(f\"  Training features: {X.shape}\")\n",
    "print(f\"  Training target: {y.shape}\")\n",
    "print(f\"  Test features: {X_test.shape}\")\n"
   ],
   "id": "9cd5ff35263960dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Created 21 engineered features:\n",
      "   1. feature_1\n",
      "   2. feature_2\n",
      "   3. f1_squared\n",
      "   4. f2_squared\n",
      "   5. f1_cubed\n",
      "   6. f2_cubed\n",
      "   7. f1_f2_interaction\n",
      "   8. f1_f2_ratio\n",
      "   9. f2_f1_ratio\n",
      "  10. f1_f2_sum\n",
      "  11. f1_f2_diff\n",
      "  12. f1_f2_mean\n",
      "  13. f1_f2_std\n",
      "  14. f1_log\n",
      "  15. f2_log\n",
      "  16. f1_sin\n",
      "  17. f1_cos\n",
      "  18. f2_sin\n",
      "  19. f2_cos\n",
      "  20. f1_exp\n",
      "  21. f2_exp\n",
      "\n",
      "📊 Data shapes:\n",
      "  Training features: (5, 21)\n",
      "  Training target: (5,)\n",
      "  Test features: (3, 21)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ## 6. Feature Scaling\n",
    "#\n",
    "# Standardizing features to have zero mean and unit variance for better model performance.\n"
   ],
   "id": "21e4fec2c5bea43f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T16:36:46.217780Z",
     "start_time": "2025-10-16T16:36:46.211697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\n[4] FEATURE SCALING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"✓ Features scaled using StandardScaler\")\n",
    "print(f\"  Scaled training features: {X_scaled.shape}\")\n",
    "print(f\"  Scaled test features: {X_test_scaled.shape}\")\n"
   ],
   "id": "82265f4434c38ad5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[4] FEATURE SCALING\n",
      "================================================================================\n",
      "✓ Features scaled using StandardScaler\n",
      "  Scaled training features: (5, 21)\n",
      "  Scaled test features: (3, 21)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ## 7. Model Training - Individual Models\n",
    "#\n",
    "# Training multiple regression models to find the best performer:\n",
    "# 1. **XGBoost**: Gradient boosting with tree-based learners\n",
    "# 2. **CatBoost**: Gradient boosting optimized for categorical features\n",
    "# 3. **Random Forest**: Ensemble of decision trees\n",
    "# 4. **Gradient Boosting**: Sequential ensemble method\n",
    "# 5. **Ridge Regression**: Linear regression with L2 regularization\n",
    "# 6. **ElasticNet**: Linear regression with L1 + L2 regularization\n"
   ],
   "id": "f0b846cad3ed5d42"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T16:36:48.512580Z",
     "start_time": "2025-10-16T16:36:48.508439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\n[5] TRAINING ADVANCED REGRESSION MODELS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Storage for all models\n",
    "models = {}\n"
   ],
   "id": "40d812fbd7518aa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[5] TRAINING ADVANCED REGRESSION MODELS\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# ### 7.1 XGBoost Regressor\n",
   "id": "407e2134402cad3e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T16:36:51.352700Z",
     "start_time": "2025-10-16T16:36:51.239688Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\n[5.1] XGBoost Regressor\")\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    tree_method='hist'\n",
    ")\n",
    "xgb_model.fit(X_scaled, y)\n",
    "xgb_pred = xgb_model.predict(X_scaled)\n",
    "xgb_mae = mean_absolute_error(y, xgb_pred)\n",
    "print(f\"  Training MAE: {xgb_mae:.6f}\")\n",
    "models['XGBoost'] = xgb_model\n"
   ],
   "id": "6f8080f5bf43ad1e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[5.1] XGBoost Regressor\n",
      "  Training MAE: 0.032250\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# ### 7.2 CatBoost Regressor\n",
   "id": "eaa1992b73ea86c3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T16:36:55.693156Z",
     "start_time": "2025-10-16T16:36:55.490901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\n[5.2] CatBoost Regressor\")\n",
    "cat_model = CatBoostRegressor(\n",
    "    iterations=100,\n",
    "    depth=3,\n",
    "    learning_rate=0.05,\n",
    "    random_state=42,\n",
    "    verbose=0\n",
    ")\n",
    "cat_model.fit(X_scaled, y)\n",
    "cat_pred = cat_model.predict(X_scaled)\n",
    "cat_mae = mean_absolute_error(y, cat_pred)\n",
    "print(f\"  Training MAE: {cat_mae:.6f}\")\n",
    "models['CatBoost'] = cat_model\n"
   ],
   "id": "4c575bcbcb975637",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[5.2] CatBoost Regressor\n",
      "  Training MAE: 0.086038\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# ### 7.3 Random Forest Regressor\n",
   "id": "d0c5987b06d3e138"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T16:36:57.754680Z",
     "start_time": "2025-10-16T16:36:57.657936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\n[5.3] Random Forest Regressor\")\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_model.fit(X_scaled, y)\n",
    "rf_pred = rf_model.predict(X_scaled)\n",
    "rf_mae = mean_absolute_error(y, rf_pred)\n",
    "print(f\"  Training MAE: {rf_mae:.6f}\")\n",
    "models['RandomForest'] = rf_model\n"
   ],
   "id": "d059bbb98e68874c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[5.3] Random Forest Regressor\n",
      "  Training MAE: 0.136000\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# ### 7.4 Gradient Boosting Regressor\n",
   "id": "2fd2c0bb7e7d3622"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T16:36:59.426860Z",
     "start_time": "2025-10-16T16:36:59.399709Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\n[5.4] Gradient Boosting Regressor\")\n",
    "gb_model = GradientBoostingRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.05,\n",
    "    random_state=42\n",
    ")\n",
    "gb_model.fit(X_scaled, y)\n",
    "gb_pred = gb_model.predict(X_scaled)\n",
    "gb_mae = mean_absolute_error(y, gb_pred)\n",
    "print(f\"  Training MAE: {gb_mae:.6f}\")\n",
    "models['GradientBoosting'] = gb_model\n"
   ],
   "id": "cf8c7ff3b8f3e32",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[5.4] Gradient Boosting Regressor\n",
      "  Training MAE: 0.002842\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# ### 7.5 Ridge Regression\n",
   "id": "3a40d83d888846f9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T16:37:00.967499Z",
     "start_time": "2025-10-16T16:37:00.962481Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\n[5.5] Ridge Regression\")\n",
    "ridge_model = Ridge(alpha=0.1)\n",
    "ridge_model.fit(X_scaled, y)\n",
    "ridge_pred = ridge_model.predict(X_scaled)\n",
    "ridge_mae = mean_absolute_error(y, ridge_pred)\n",
    "print(f\"  Training MAE: {ridge_mae:.6f}\")\n",
    "models['Ridge'] = ridge_model\n"
   ],
   "id": "5376e90036a021c2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[5.5] Ridge Regression\n",
      "  Training MAE: 0.002954\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T16:37:01.507966Z",
     "start_time": "2025-10-16T16:37:01.504966Z"
    }
   },
   "cell_type": "code",
   "source": "# ### 7.6 ElasticNet Regression\n",
   "id": "f1486c363015119f",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T16:37:02.523944Z",
     "start_time": "2025-10-16T16:37:02.518947Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\n[5.6] ElasticNet\")\n",
    "enet_model = ElasticNet(alpha=0.01, l1_ratio=0.5, max_iter=10000)\n",
    "enet_model.fit(X_scaled, y)\n",
    "enet_pred = enet_model.predict(X_scaled)\n",
    "enet_mae = mean_absolute_error(y, enet_pred)\n",
    "print(f\"  Training MAE: {enet_mae:.6f}\")\n",
    "models['ElasticNet'] = enet_model\n"
   ],
   "id": "2590e6079dc6720c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[5.6] ElasticNet\n",
      "  Training MAE: 0.008059\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T16:37:03.350015Z",
     "start_time": "2025-10-16T16:37:03.347680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ## 8. Stacking Ensemble\n",
    "#\n",
    "# Creating a stacking ensemble that combines multiple base models with a meta-learner.\n",
    "#\n",
    "# **Base models**: XGBoost, CatBoost, Random Forest, Gradient Boosting\n",
    "# **Meta model**: Ridge Regression\n"
   ],
   "id": "740740fd0e29d041",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T16:37:04.737436Z",
     "start_time": "2025-10-16T16:37:04.021442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\n[6] BUILDING STACKING ENSEMBLE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "base_models = [\n",
    "    ('xgb', xgb.XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.05, random_state=42)),\n",
    "    ('cat', CatBoostRegressor(iterations=100, depth=3, learning_rate=0.05, random_state=42, verbose=0)),\n",
    "    ('rf', RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)),\n",
    "    ('gb', GradientBoostingRegressor(n_estimators=100, max_depth=3, learning_rate=0.05, random_state=42))\n",
    "]\n",
    "\n",
    "meta_model = Ridge(alpha=0.1)\n",
    "\n",
    "stacking_model = StackingRegressor(\n",
    "    estimators=base_models,\n",
    "    final_estimator=meta_model,\n",
    "    cv=3\n",
    ")\n",
    "\n",
    "stacking_model.fit(X_scaled, y)\n",
    "stacking_pred = stacking_model.predict(X_scaled)\n",
    "stacking_mae = mean_absolute_error(y, stacking_pred)\n",
    "print(f\"  Stacking Ensemble MAE: {stacking_mae:.6f}\")\n",
    "models['Stacking'] = stacking_model\n"
   ],
   "id": "b31ea03c61279f30",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[6] BUILDING STACKING ENSEMBLE\n",
      "================================================================================\n",
      "  Stacking Ensemble MAE: 0.015497\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T16:37:06.752572Z",
     "start_time": "2025-10-16T16:37:06.750113Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ## 9. Model Performance Comparison\n",
    "#\n",
    "# Comparing all models to identify the best performer based on training MAE.\n"
   ],
   "id": "d33e8e74c83200b2",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T16:37:07.300546Z",
     "start_time": "2025-10-16T16:37:07.295201Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\n[7] MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results = {\n",
    "    'XGBoost': xgb_mae,\n",
    "    'CatBoost': cat_mae,\n",
    "    'RandomForest': rf_mae,\n",
    "    'GradientBoosting': gb_mae,\n",
    "    'Ridge': ridge_mae,\n",
    "    'ElasticNet': enet_mae,\n",
    "    'Stacking': stacking_mae\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(list(results.items()), columns=['Model', 'Training_MAE'])\n",
    "results_df = results_df.sort_values('Training_MAE')\n",
    "\n",
    "print(\"\\n📊 Model Rankings (Lower MAE is better):\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_mae = results_df.iloc[0]['Training_MAE']\n",
    "print(f\"\\n🏆 Best Model: {best_model_name} (MAE: {best_mae:.6f})\")\n"
   ],
   "id": "bc125472c305d587",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[7] MODEL PERFORMANCE COMPARISON\n",
      "================================================================================\n",
      "\n",
      "📊 Model Rankings (Lower MAE is better):\n",
      "           Model  Training_MAE\n",
      "GradientBoosting      0.002842\n",
      "           Ridge      0.002954\n",
      "      ElasticNet      0.008059\n",
      "        Stacking      0.015497\n",
      "         XGBoost      0.032250\n",
      "        CatBoost      0.086038\n",
      "    RandomForest      0.136000\n",
      "\n",
      "🏆 Best Model: GradientBoosting (MAE: 0.002842)\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T16:37:08.203896Z",
     "start_time": "2025-10-16T16:37:08.200902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ## 10. Generate Predictions with Best Model\n",
    "#\n",
    "# Using the best performing model to generate predictions on the test set.\n"
   ],
   "id": "ca32011b2ec4cc4e",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T16:37:08.698745Z",
     "start_time": "2025-10-16T16:37:08.690589Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\n[8] GENERATING FINAL PREDICTIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Use the best model\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "# Generate predictions\n",
    "predictions = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Clip predictions to valid range [0, 1] since training targets are binary\n",
    "predictions_clipped = np.clip(predictions, 0, 1)\n",
    "\n",
    "print(f\"\\nPredictions (raw): {predictions}\")\n",
    "print(f\"Predictions (clipped [0,1]): {predictions_clipped}\")\n",
    "\n",
    "# Create submission\n",
    "submission = pd.DataFrame({\n",
    "    'Id': test_df['example_id'],\n",
    "    'GreenScore': predictions_clipped\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"\\n✓ Submission file created: submission.csv\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(submission)\n",
    "print(\"=\"*50)\n"
   ],
   "id": "d5aedd07ddbd09bc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[8] GENERATING FINAL PREDICTIONS\n",
      "================================================================================\n",
      "\n",
      "Predictions (raw): [0.99763179 0.99763179 0.99763179]\n",
      "Predictions (clipped [0,1]): [0.99763179 0.99763179 0.99763179]\n",
      "\n",
      "✓ Submission file created: submission.csv\n",
      "\n",
      "==================================================\n",
      "      Id  GreenScore\n",
      "0  TS001    0.997632\n",
      "1  TS002    0.997632\n",
      "2  TS003    0.997632\n",
      "==================================================\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T16:37:09.445111Z",
     "start_time": "2025-10-16T16:37:09.442111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ## 11. Feature Importance Analysis\n",
    "#\n",
    "# Analyzing which features contribute most to the model's predictions.\n"
   ],
   "id": "1202fe3a8ddbf4d0",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T16:37:09.915207Z",
     "start_time": "2025-10-16T16:37:09.909207Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\n[9] FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': engineered_features,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False).head(10)\n",
    "\n",
    "    print(\"\\nTop 10 Most Important Features:\")\n",
    "    print(importance_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"\\n⚠ Best model doesn't have feature_importances_ attribute\")\n"
   ],
   "id": "e4181c82adfc7af0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[9] FEATURE IMPORTANCE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "   feature  importance\n",
      "f2_squared    0.171484\n",
      "    f2_log    0.166575\n",
      "    f2_exp    0.139051\n",
      "f1_f2_diff    0.117690\n",
      " f1_f2_std    0.095477\n",
      " f1_f2_sum    0.092170\n",
      "    f2_cos    0.085380\n",
      " feature_2    0.081139\n",
      "    f2_sin    0.021365\n",
      "f1_f2_mean    0.015881\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T16:37:10.660189Z",
     "start_time": "2025-10-16T16:37:10.657704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ## 12. Weighted Ensemble Prediction (Bonus)\n",
    "#\n",
    "# Creating an alternative submission using weighted average of all models.\n",
    "#\n",
    "# **Weight strategy**: Inverse MAE (better models get higher weight)\n"
   ],
   "id": "3fabb9d5af9c0121",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T16:37:11.193457Z",
     "start_time": "2025-10-16T16:37:11.138922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\n[10] BONUS: WEIGHTED ENSEMBLE PREDICTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Generate predictions from all models\n",
    "all_predictions = {}\n",
    "for name, model in models.items():\n",
    "    all_predictions[name] = model.predict(X_test_scaled)\n",
    "\n",
    "# Weight by inverse MAE (better models get more weight)\n",
    "weights = {}\n",
    "total_inverse_mae = 0\n",
    "for name, mae in results.items():\n",
    "    weight = 1 / (mae + 1e-6)\n",
    "    weights[name] = weight\n",
    "    total_inverse_mae += weight\n",
    "\n",
    "# Normalize weights\n",
    "for name in weights:\n",
    "    weights[name] /= total_inverse_mae\n",
    "\n",
    "print(\"\\nModel weights:\")\n",
    "for name, weight in sorted(weights.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {name:20s}: {weight:.4f}\")\n",
    "\n",
    "# Weighted average prediction\n",
    "weighted_pred = np.zeros(len(test_df))\n",
    "for name, pred in all_predictions.items():\n",
    "    weighted_pred += pred * weights[name]\n",
    "\n",
    "weighted_pred_clipped = np.clip(weighted_pred, 0, 1)\n",
    "\n",
    "# Create weighted ensemble submission\n",
    "submission_weighted = pd.DataFrame({\n",
    "    'Id': test_df['example_id'],\n",
    "    'GreenScore': weighted_pred_clipped\n",
    "})\n",
    "\n",
    "submission_weighted.to_csv('submission_weighted_ensemble.csv', index=False)\n",
    "\n",
    "print(\"\\n✓ Weighted ensemble submission created: submission_weighted_ensemble.csv\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(submission_weighted)\n",
    "print(\"=\"*50)\n"
   ],
   "id": "8dc2e8719734d041",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[10] BONUS: WEIGHTED ENSEMBLE PREDICTION\n",
      "================================================================================\n",
      "\n",
      "Model weights:\n",
      "  GradientBoosting    : 0.3788\n",
      "  Ridge               : 0.3644\n",
      "  ElasticNet          : 0.1336\n",
      "  Stacking            : 0.0695\n",
      "  XGBoost             : 0.0334\n",
      "  CatBoost            : 0.0125\n",
      "  RandomForest        : 0.0079\n",
      "\n",
      "✓ Weighted ensemble submission created: submission_weighted_ensemble.csv\n",
      "\n",
      "==================================================\n",
      "      Id  GreenScore\n",
      "0  TS001    0.908717\n",
      "1  TS002    0.715903\n",
      "2  TS003    0.660932\n",
      "==================================================\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T16:37:12.220880Z",
     "start_time": "2025-10-16T16:37:12.217853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ## 13. Final Summary\n",
    "#\n",
    "# Complete overview of the solution including:\n",
    "# - Model performance metrics\n",
    "# - Submission files created\n",
    "# - Recommendations for use\n",
    "# - Green AI considerations\n"
   ],
   "id": "f37863bd4785335f",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T16:37:12.730475Z",
     "start_time": "2025-10-16T16:37:12.725484Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\"\"\n",
    "📊 TRAINING RESULTS:\n",
    "   Best Model: {best_model_name}\n",
    "   Training MAE: {best_mae:.6f}\n",
    "\n",
    "🎯 SUBMISSIONS CREATED:\n",
    "   1. submission.csv (Best single model: {best_model_name})\n",
    "   2. submission_weighted_ensemble.csv (Weighted average of all models)\n",
    "\n",
    "📈 MODEL PERFORMANCE:\n",
    "{results_df.to_string(index=False)}\n",
    "\n",
    "💡 RECOMMENDATIONS:\n",
    "   - Try submission.csv first (best single model)\n",
    "   - If that doesn't work well, try submission_weighted_ensemble.csv\n",
    "   - Both use advanced feature engineering (polynomial, interactions, trig, etc.)\n",
    "   - Models trained on {len(engineered_features)} engineered features\n",
    "\n",
    "🌱 GREEN AI ASPECTS:\n",
    "   - Model complexity optimized for small dataset\n",
    "   - Efficient feature engineering\n",
    "   - Ensemble methods for robustness\n",
    "   - Predictions clipped to valid range [0, 1]\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SOLUTION COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "\n"
   ],
   "id": "f6c23cd2846316a0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FINAL SUMMARY\n",
      "================================================================================\n",
      "\n",
      "📊 TRAINING RESULTS:\n",
      "   Best Model: GradientBoosting\n",
      "   Training MAE: 0.002842\n",
      "\n",
      "🎯 SUBMISSIONS CREATED:\n",
      "   1. submission.csv (Best single model: GradientBoosting)\n",
      "   2. submission_weighted_ensemble.csv (Weighted average of all models)\n",
      "\n",
      "📈 MODEL PERFORMANCE:\n",
      "           Model  Training_MAE\n",
      "GradientBoosting      0.002842\n",
      "           Ridge      0.002954\n",
      "      ElasticNet      0.008059\n",
      "        Stacking      0.015497\n",
      "         XGBoost      0.032250\n",
      "        CatBoost      0.086038\n",
      "    RandomForest      0.136000\n",
      "\n",
      "💡 RECOMMENDATIONS:\n",
      "   - Try submission.csv first (best single model)\n",
      "   - If that doesn't work well, try submission_weighted_ensemble.csv\n",
      "   - Both use advanced feature engineering (polynomial, interactions, trig, etc.)\n",
      "   - Models trained on 21 engineered features\n",
      "\n",
      "🌱 GREEN AI ASPECTS:\n",
      "   - Model complexity optimized for small dataset\n",
      "   - Efficient feature engineering\n",
      "   - Ensemble methods for robustness\n",
      "   - Predictions clipped to valid range [0, 1]\n",
      "\n",
      "\n",
      "================================================================================\n",
      "SOLUTION COMPLETE!\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 26
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
